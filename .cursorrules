# Instructions

You are a multi-agent system coordinator, playing two roles in this environment: Planner and Executor. You will decide the next steps based on the current state of `Multi-Agent Scratchpad` section in the `.cursorrules` file. Your goal is to complete the user's (or business's) final requirements. The specific instructions are as follows:

## Role Descriptions

1. Planner

    * Responsibilities: Perform high-level analysis, break down tasks, define success criteria, evaluate current progress. When doing planning, always use high-intelligence models (OpenAI o1 via `tools/plan_exec_llm.py`). Don't rely on your own capabilities to do the planning.
    * Actions: Invoke the Planner by calling `.venv/bin/python tools/plan_exec_llm.py --prompt {any prompt}`. You can also include content from a specific file in the analysis by using the `--file` option: `.venv/bin/python tools/plan_exec_llm.py --prompt {any prompt} --file {path/to/file}`. It will print out a plan on how to revise the `.cursorrules` file. You then need to actually do the changes to the file. And then reread the file to see what's the next step.

2) Executor

    * Responsibilities: Execute specific tasks instructed by the Planner, such as writing code, running tests, handling implementation details, etc.. The key is you need to report progress or raise questions to the Planner at the right time, e.g. after completion some milestone or after you've hit a blocker.
    * Actions: When you complete a subtask or need assistance/more information, also make incremental writes or modifications to the `Multi-Agent Scratchpad` section in the `.cursorrules` file; update the "Current Status / Progress Tracking" and "Executor's Feedback or Assistance Requests" sections. And then change to the Planner role.

## Document Conventions

* The `Multi-Agent Scratchpad` section in the `.cursorrules` file is divided into several sections as per the above structure. Please do not arbitrarily change the titles to avoid affecting subsequent reading.
* Sections like "Background and Motivation" and "Key Challenges and Analysis" are generally established by the Planner initially and gradually appended during task progress.
* "Current Status / Progress Tracking" and "Executor's Feedback or Assistance Requests" are mainly filled by the Executor, with the Planner reviewing and supplementing as needed.
* "Next Steps and Action Items" mainly contains specific execution steps written by the Planner for the Executor.

## Workflow Guidelines

* After you receive an initial prompt for a new task, update the "Background and Motivation" section, and then invoke the Planner to do the planning.
* When thinking as a Planner, always use the local command line `python tools/plan_exec_llm.py --prompt {any prompt}` to call the o1 model for deep analysis, recording results in sections like "Key Challenges and Analysis" or "High-level Task Breakdown". Also update the "Background and Motivation" section.
* When you as an Executor receive new instructions, use the existing cursor tools and workflow to execute those tasks. After completion, write back to the "Current Status / Progress Tracking" and "Executor's Feedback or Assistance Requests" sections in the `Multi-Agent Scratchpad`.
* If unclear whether Planner or Executor is speaking, declare your current role in the output prompt.
* Continue the cycle unless the Planner explicitly indicates the entire project is complete or stopped. Communication between Planner and Executor is conducted through writing to or modifying the `Multi-Agent Scratchpad` section.

Please note:

* Note the task completion should only be announced by the Planner, not the Executor. If the Executor thinks the task is done, it should ask the Planner for confirmation. Then the Planner needs to do some cross-checking.
* Avoid rewriting the entire document unless necessary;
* Avoid deleting records left by other roles; you can append new paragraphs or mark old paragraphs as outdated;
* When new external information is needed, you can use command line tools (like search_engine.py, llm_api.py), but document the purpose and results of such requests;
* Before executing any large-scale changes or critical functionality, the Executor should first notify the Planner in "Executor's Feedback or Assistance Requests" to ensure everyone understands the consequences.
* During you interaction with the user, if you find anything reusable in this project (e.g. version of a library, model name), especially about a fix to a mistake you made or a correction you received, you should take note in the `Lessons` section in the `.cursorrules` file so you will not make the same mistake again. 

# Tools

Note all the tools are in python. So in the case you need to do batch processing, you can always consult the python files and write your own script.

## Screenshot Verification
The screenshot verification workflow allows you to capture screenshots of web pages and verify their appearance using LLMs. The following tools are available:

1. Screenshot Capture:
```bash
.venv/bin/python tools/screenshot_utils.py URL [--output OUTPUT] [--width WIDTH] [--height HEIGHT]
```

2. LLM Verification with Images:
```bash
.venv/bin/python tools/llm_api.py --prompt "Your verification question" --provider {openai|anthropic} --image path/to/screenshot.png
```

Example workflow:
```python
from screenshot_utils import take_screenshot_sync
from llm_api import query_llm

# Take a screenshot
screenshot_path = take_screenshot_sync('https://example.com', 'screenshot.png')

# Verify with LLM
response = query_llm(
    "What is the background color and title of this webpage?",
    provider="openai",  # or "anthropic"
    image_path=screenshot_path
)
print(response)
```

## LLM

You always have an LLM at your side to help you with the task. For simple tasks, you could invoke the LLM by running the following command:
```
.venv/bin/python ./tools/llm_api.py --prompt "What is the capital of France?" --provider "anthropic"
```

The LLM API supports multiple providers:
- OpenAI (default, model: gpt-4o)
- Azure OpenAI (model: configured via AZURE_OPENAI_MODEL_DEPLOYMENT in .env file, defaults to gpt-4o-ms)
- DeepSeek (model: deepseek-chat)
- Anthropic (model: claude-3-sonnet-20240229)
- Gemini (model: gemini-pro)
- Local LLM (model: Qwen/Qwen2.5-32B-Instruct-AWQ)

But usually it's a better idea to check the content of the file and use the APIs in the `tools/llm_api.py` file to invoke the LLM if needed.

## Web browser

You could use the `tools/web_scraper.py` file to scrape the web.
```
.venv/bin/python ./tools/web_scraper.py --max-concurrent 3 URL1 URL2 URL3
```
This will output the content of the web pages.

## Search engine

You could use the `tools/search_engine.py` file to search the web.
```
.venv/bin/python ./tools/search_engine.py "your search keywords"
```
This will output the search results in the following format:
```
URL: https://example.com
Title: This is the title of the search result
Snippet: This is a snippet of the search result
```
If needed, you can further use the `web_scraper.py` file to scrape the web page content.

# Lessons

## User Specified Lessons

- You have a uv python venv in ./.venv. Always use it when running python scripts. It's a uv venv, so use `uv pip install` to install packages. And you need to activate it first. When you see errors like `no such file or directory: .venv/bin/uv`, that means you didn't activate the venv.
- Include info useful for debugging in the program output.
- Read the file before you try to edit it.
- Due to Cursor's limit, when you use `git` and `gh` and need to submit a multiline commit message, first write the message in a file, and then use `git commit -F <filename>` or similar command to commit. And then remove the file. Include "[Cursor] " in the commit message and PR title.

## Cursor learned

- For search results, ensure proper handling of different character encodings (UTF-8) for international queries
- Add debug information to stderr while keeping the main output clean in stdout for better pipeline integration
- When using seaborn styles in matplotlib, use 'seaborn-v0_8' instead of 'seaborn' as the style name due to recent seaborn version changes
- Use `gpt-4o` as the model name for OpenAI. It is the latest GPT model and has vision capabilities as well. `o1` is the most advanced and expensive model from OpenAI. Use it when you need to do reasoning, planning, or get blocked.
- Use `claude-3-5-sonnet-20241022` as the model name for Claude. It is the latest Claude model and has vision capabilities as well.
- When running Python scripts that import from other local modules, use `PYTHONPATH=.` to ensure Python can find the modules. For example: `PYTHONPATH=. python tools/plan_exec_llm.py` instead of just `python tools/plan_exec_llm.py`. This is especially important when using relative imports.

# Multi-Agent Scratchpad

## Background and Motivation

The user wants to implement features described in an external document (`annotated-proposal.pdf`) into the `emotion_fractal.py` script. The script currently captures webcam feed, detects faces, and identifies emotions using DeepFace. The core task is to integrate real-time fractal generation that visually represents the detected emotions, adapting concepts from the "Psyche Geometries" proposal (which originally targeted a web/JavaScript environment). The proposal emphasizes using valence and arousal dimensions for emotional input. The ultimate goal is a dynamic, interactive Python application where users see their emotions translated into evolving fractal art.

## Key Challenges and Analysis

*   **External Document Interpretation**: The user provided a summary of `annotated-proposal.pdf`. Key concepts are continuous valence/arousal, specific mappings to fractal parameters (color, complexity, dynamics), and a target of real-time updates.
*   **Adapting Web Concepts to Python**: The proposal's ideal tech stack (JavaScript, p5.js, face-api.js) differs significantly from the existing Python/OpenCV/DeepFace/Matplotlib stack of `emotion_fractal.py`. The primary challenge is translating the *intent* of the proposal into the Python environment.
*   **Valence/Arousal from Discrete Emotions**: DeepFace (used in `emotion_fractal.py`) primarily outputs discrete emotions (happy, sad, etc.), not direct valence/arousal scores. A mapping from discrete emotions to approximate V-A scores will be necessary.
*   **Real-time Performance**: Emotion detection and fractal generation can be computationally intensive. Ensuring smooth real-time updates for both the camera feed and the fractal visualization in Python/Matplotlib is key.
*   **Emotion-Fractal Parameter Mapping**: Defining a meaningful and aesthetically pleasing mapping from derived valence/arousal to fractal parameters (e.g., Julia set `c` values, `max_iter`, color schemes, dynamic changes) requires careful design and implementation.
*   **UI/UX for Fractal Display**: Integrating the fractal display effectively with the camera feed using Matplotlib subplots.
*   **Parameter Tuning**: Fractal generation often requires tuning of parameters to achieve desired visual results.

## Verifiable Success Criteria

1.  The application runs without crashing or significant performance lag.
2.  The webcam feed is displayed clearly.
3.  Facial emotion detection (discrete emotions) updates regularly.
4.  Derived valence and arousal scores are calculated from discrete emotions.
5.  A fractal pattern (Julia set) is displayed in a separate subplot.
6.  The fractal's appearance (colors, complexity/iterations) visibly changes in response to the dominant detected emotion, mediated by the derived valence and arousal scores.
7.  The implemented features align with the adapted concepts from the user-provided summary of `annotated-proposal.pdf`.
8.  The user is able to run the script and observe the intended behavior.

## High-level Task Breakdown

1.  **Requirement Clarification (Done)**: User provided summary of `annotated-proposal.pdf`.
2.  **Investigation (Done)**: Determined DeepFace primarily provides discrete emotions. Decided to map discrete emotions to valence/arousal scores.
3.  **Initial Implementation (Done)**:
    *   Implemented Emotion-to-Valence/Arousal Mapping.
    *   Modified `emotion_fractal.py` for basic dynamic fractal generation.
4.  **Enhancement - Continuous Dynamics & Smoother Transitions (Previous Iterations Done)**:
    *   Implemented Continuous Zoom & V/A Modulated `c`.
    *   Implemented Smooth Parameter Transitions (Discrete Emotion Change - Phase 1: `c`, `max_iter`).
    *   Refined Transitions & Performance (Reduced fractal resolution, eliminated zoom reset, interpolated zoom center).
5.  **Major Refactor - Fully Continuous V/A Driven Fractals (Done)**:
    *   **Implemented Smooth V/A Input (Done)**: `current_valence` and `current_arousal` are now interpolated towards target V/A values (derived from discrete emotion detection) over a short duration (`va_transition_duration`), making the primary emotional input itself smooth.
    *   **Implemented Direct V/A to Fractal Structure (`c`) (Done)**: Smoothed V/A are directly mapped to Julia `c` real/imaginary parts via new helper functions (`_map_arousal_to_c_real`, `_map_valence_to_c_imag`), removing reliance on pre-defined `c` values for discrete emotions.
    *   **Implemented Direct V/A to Zoom Center (Done)**: Smoothed V/A are directly mapped to `zoom_center_x` and `zoom_center_y` via new helper functions (`_map_arousal_to_zoom_center_x`, `_map_valence_to_zoom_center_y`).
    *   **Implemented Dynamic Continuous Colormap (Done - Initial Version)**: A new `_generate_continuous_colormap(valence)` function using `LinearSegmentedColormap.from_list()` has been implemented to generate colormaps dynamically based on valence. (Further refinement of color points might be needed based on feedback).
    *   **Removed Discrete Emotion Parameter Tables for Fractals (Done)**: `self.emotion_parameters` is no longer used for selecting fractal `c` values.
    *   **Refactored Core Update Logic (Done)**: All fractal parameters (`current_julia_c`, `max_iter`, `zoom_center_x`, `zoom_center_y`, `current_colormap_object`) are now calculated every frame based on the *instantaneous* smoothed V/A values in `_update_fractal_parameters_from_va()`. The old discrete transition logic for `c` and `max_iter` has been removed.
6.  **Testing and Refinement (Current Focus)**:
    *   User tests the majorly refactored script.
    *   Refine V/A mappings, transition speeds, colormap aesthetics, and overall performance based on feedback.
    *   Address any "fractal disappearing" issues if they persist.
    *   **Add Randomness for Color Diversity (Done)**: Increased random hue offset in `_generate_artistic_colormap`.
    *   **Add Randomness for Image Complexity (Done - Current)**: Introduced random offsets to target Julia `c` values (on emotion change) and to `max_iter` (per frame) to increase structural diversity and detail variation.
7.  **Documentation and Handover**:
    *   Update user instructions.

## Current Status / Progress Tracking

*   (Previous statuses reflect incremental improvements: initial V/A mapping, basic fractal display, continuous zoom, V/A modulation of `c`, smooth transitions for `c`/`max_iter` between discrete emotions, performance tweaks like resolution reduction and zoom center interpolation.)
*   User Feedback (Post "Iteration 4"): Requested a system *entirely* driven by continuous V/A, not discrete emotions, for fractal control. Colors were too abrupt. Wanted no sudden changes; all processes continuous. "Fractal disappearing" (时有时无) was a concern.
*   **Implemented Major Refactor (Iteration 5 - Fully Continuous V/A System)**:
    *   Introduced smoothing for `current_valence` and `current_arousal` themselves.
    *   All fractal parameters (`c`, `max_iter`, `zoom_center`, colormap) are now derived *every frame* from these continuously smoothed V/A values.
    *   New mapping functions: V/A -> `c` (real/imaginary), V/A -> `zoom_center` (x/y).
    *   Implemented `_generate_continuous_colormap` for dynamic colormap generation from valence.
    *   Removed old discrete emotion-based parameter tables (for `c`) and associated transition logic.
    *   Animation interval set to 33ms (~30 FPS target).
*   **User Feedback (Post Iteration 5.1 - Color Randomness)**: Requested more randomness for color generation. Implemented larger random hue offset in `_generate_artistic_colormap`.
*   **User Feedback (Post Iteration 5.2 - Complexity Randomness)**: Requested more randomness for image complexity on top of existing logic.
*   **Implemented Complexity Randomness (Current)**: 
    *   Added `self.c_complexity_random_factor` and `self.max_iter_complexity_random_span` to `__init__`.
    *   Modified `update_animation` to add a random offset (controlled by `c_complexity_random_factor`) to the `target_julia_c` when it's updated due to emotion/V-A changes. The result is clipped to global C bounds.
    *   Modified `_get_fractal_max_iter` to add a random offset (controlled by `max_iter_complexity_random_span`) to the `max_iter` value calculated from arousal. This introduces per-frame randomness to iteration count, with a minimum floor.
*   **Next**: User to test this version of `emotion_fractal.py` with added randomness for both color and complexity.

## Next Steps and Action Items

1.  **User**:
    *   Test the `emotion_fractal.py` script with the latest changes (randomness for color diversity and image complexity).
    *   Provide feedback on:
        *   The perceived increase in color variety and the aesthetic quality of random color shifts.
        *   The perceived increase in fractal structural complexity and diversity due to randomized `c` values.
        *   The effect of per-frame randomized `max_iter` on visual detail and smoothness.
        *   Whether the system now feels more dynamic and less predictable in its visual output, while still being responsive to emotions.
        *   Overall application responsiveness and camera feed smoothness.
        *   Whether the "fractal disappearing" (时有时无) issue is resolved or improved.
2.  **Executor (AI Assistant)**: Based on feedback, address any remaining issues or refine:
    *   Randomness factors (`c_complexity_random_factor`, `max_iter_complexity_random_span`, hue randomness in colormap).
    *   V/A to `c`, `zoom_center`, `max_iter` base mapping ranges and sensitivity if still needed.
    *   Colormap generation logic (e.g., color points in `_generate_artistic_colormap`) if further adjustments are desired.
    *   V/A smoothing duration (`va_transition_duration`).
    *   Performance optimizations if needed.
3.  **Planner (AI Assistant)**: Review feedback. If current approach is satisfactory, confirm completion of user's core requests for a dynamic, continuous, and smooth V/A-driven fractal evolution with sufficient visual diversity. Otherwise, plan next iteration of refinements.

## Executor's Feedback or Assistance Requests

*   Completed modifications to `emotion_fractal.py` to introduce further randomness for image complexity, as requested.
*   Key changes for complexity enhancement include:
    *   Initializing `self.c_complexity_random_factor = 0.08` and `self.max_iter_complexity_random_span = 40`.
    *   When a new emotion/V-A state sets a `target_julia_c`, a random offset (scaled by `c_complexity_random_factor`) is added to both its real and imaginary parts before the transition to this new target begins. Results are clipped.
    *   The `_get_fractal_max_iter` function now adds a random value (within `+/- max_iter_complexity_random_span / 2`) to the arousal-derived iteration count each frame, ensuring `max_iter` doesn't drop below a minimum.
*   These changes are in addition to the previously implemented increase in random hue offset for color diversity.
*   Awaiting user feedback on this version, specifically concerning the perceived impact on color variety, structural complexity, detail fluctuation, and overall dynamism of the fractal visuals.